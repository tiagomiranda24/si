{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the layer with a random input and check if the output shows the desired behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from si.neural_networks.layers import DropoutLayer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "\n",
      "Input:\n",
      "[[ 0.2601804  -0.59984995  0.78609914]\n",
      " [ 0.01152746  1.5452114   1.08075114]\n",
      " [ 0.25134991  0.15358925  1.03383439]]\n",
      "\n",
      "Output with Dropout (during training):\n",
      "[[ 0.52036079 -0.          0.        ]\n",
      " [ 0.02305492  0.          0.        ]\n",
      " [ 0.50269981  0.          0.        ]]\n",
      "\n",
      "Applied Mask (0 values represent deactivated neurons):\n",
      "[[1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Dropout rate (50%)\n",
    "probability = 0.5  \n",
    "# Random input data (3 samples, 3 features)\n",
    "input_data = np.random.randn(3, 3)  \n",
    "\n",
    "# Create the Dropout layer\n",
    "dropout_layer = DropoutLayer(probability=probability)\n",
    "\n",
    "# Testing during Training:\n",
    "print(\"Training:\")\n",
    "output_train = dropout_layer.forward_propagation(input_data, training=True)\n",
    "\n",
    "# Display the input and the output after applying Dropout\n",
    "print(\"\\nInput:\")\n",
    "print(input_data)\n",
    "print(\"\\nOutput with Dropout (during training):\")\n",
    "print(output_train)\n",
    "print(\"\\nApplied Mask (0 values represent deactivated neurons):\")\n",
    "print(dropout_layer.mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference:\n",
      "\n",
      "Input:\n",
      "[[ 0.2601804  -0.59984995  0.78609914]\n",
      " [ 0.01152746  1.5452114   1.08075114]\n",
      " [ 0.25134991  0.15358925  1.03383439]]\n",
      "\n",
      "Output with Dropout (no changes during inference):\n",
      "[[ 0.2601804  -0.59984995  0.78609914]\n",
      " [ 0.01152746  1.5452114   1.08075114]\n",
      " [ 0.25134991  0.15358925  1.03383439]]\n"
     ]
    }
   ],
   "source": [
    "# Testing during Inference:\n",
    "print(\"\\nInference:\")\n",
    "output_infer = dropout_layer.forward_propagation(input_data, training=False)\n",
    "\n",
    "# Display the input and the output during inference (no changes, no dropout)\n",
    "print(\"\\nInput:\")\n",
    "print(input_data)\n",
    "print(\"\\nOutput with Dropout (no changes during inference):\")\n",
    "print(output_infer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the function behaves as expected. During training, after applying the mask, the output contains both deactivated (0) and activated neurons. \n",
    "However, during inference, no neurons are deactivated. The sizes of the input and output remain consistent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiago",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
